Title: Optimized computational modeling of adaptive activation-based concept learning

TODO: sequences and series + recursion



1. Introduction
    (REDO)
    Technology has made its way into our working environments, vehicles, mobile phones and even into our homes, in an attempt to make our lives easier and more convenient. One area of our lives, where technology has been gaining popularity is education. Educational technology in particular has been gaining increasing popularity due to its efforts in improving the application of technology in facilitating learning.
    Learning words in languages = very important, declarative memory? A language user's mother tongue is learned at a very young age when not much conscious effort is put into the learning process, but instead the learner acquires the language unconsciously therefore that is referred to as "language acquisition". On the other hand when it comes to learning further foreign languages, the user has to actively participate in the learning process in which case we speak of "language learning".
    Cognitive psychology has been able to identify a number of patterns, called cognitive biases, which try to explain how humans process information, such as the modality effect, the testing effect and the spacing effect. While being aware of these patterns allows educators to simply develop systems, which reliably produce improved learning results, computers require well defined and formalized rules in order to achieve this. One of the more famous and successful cognitive architectures is the Adaptive Control of Thoughtâ€”Rational (known as ACT-R).
    The goal of this thesis is to analyze an adaptive activation-based learning model, based on the ACT-R architecture and try to develop a more computationally efficient implementation of that model. I will compare the performance as well as the accuracy of the resulting implementation to the original implementation of the model.



2. Background
    Subsection: The spacing and testing effects
    When it comes to factual learning tasks, massed learning (colloquially known as cramming) has become the go-to approach. It consists of exposing the learner to a big amount of information for a short amount of time in an attempt to prepare the learner for an upcoming examination on the studied material. Research, however, has shown (Ebbinghaus, 1885) that massed learning only produces short-term results and therefore is unsuitable when we want to achieve long-term information retention. The proposed alternative for effective long-term results is the so-called distributed (or spaced) learning, which is based on Ebbinghaus' conclusions that spacing the individual item interactions further apart actually produces consistent long-term retention. This discovery is known as the "spacing effect". Furthermore, spaced learning attempts to optimize both intra-session intervals (how often each word gets presented within a study session) and inter-session intervals (how far apart different study sessions should take place).

    Subsection: The basics of ACT-R
    Since the model, which is being analyzed in this thesis is based on the ACT-R cognitive architecture, I will provide a very superficial introduction to it in order to introduce some important terminology and concepts. ACT-R stands for "Adaptive Character of Thought - Rational" and its aim is to explain and formally define how humans perceive and express both declarative and procedural knowledge. Declarative knowledge refers to the ability of the human brain to retrieve <BOLD factual information BOLD>, whereas procedural knowledge is connected to the subconscious ability to accomplish somewhat <BOLD complex procedures BOLD> (riding a bicycle, going up the stairs, writing a text etc.). Since vocabulary learning can be compared to learning facts, the model described in this thesis will make use of the implementation of declarative memory in the ACT-R architecture. The model is also referred to as an activation-based one, as it relies on the concept of "activations", which ACT-R uses in order to represent how strongly each memory item is embedded into memory. {Reference: van Woundenberg 2008}.

    Subsection: The Pavlik and Anderson model
    Pavlik and Anderson have managed to formalize this research into their model by modifying the base ACT-R formula for calculating activations. Formula 1.0 shows the formula developed by Pavlik and Anderson for calculating the activation of an item at a given time <ITALIC t ITALIC>. An item's activation is expressed as a power function of the sum of the time deltas of its previous encounters (how long ago each encounter occured), scaled by the decay rate at the given encounter (how fast the activation decays with the passage of time). This results in the latest encounters having the highest impact on the item's current activation. *{ASTERISK: what is a word encounter??? (word-pair, sentence with missing word, etc.)}
    Formula 1.0: <ACTIVATION CALCULATION FORMULA>
    Decay for the current encounter is calculated by taking the activation of the item at the last encounter (formula 1.1). If a previous encounter happened when the item's activation was high, this would also produce a high rate of decay, which when plugged into the activation formula would further scale down the time difference, resulting in a smaller contribution of that encounter to the current activation of the item (meaning that being presented with a recently encountered item has little effect when it comes to strenghtening that item in memory). Decay can further be modified using the other two variables seen in the formula. The <ITALIC c ITALIC> parameter represents a scaling factor, which governs how strong the spacing effect is and the <ITALIC ALPHA ITALIC> parameter represents the lowest possible decay value (usually 0.5 since that is the default decay value in ACT-R). The formula recurses down for each previous encounter and the base case for each item occurs when the activation is being calculated for a time <ITALIC t ITALIC> when there are no previous encounters. In that case the result is an activation of negative infinity (-INF), since the item has never been encountered before and therefore has no strength whatsoever in the learner's memory. If decay is being calculated for an activation of -INF, this results in the default decay value.
    Formula 1.1: <DECAY CALCULATION FORMULA>
    Having calculated the activations of all items in the current vocabulary list at the current time, the model decides which item to present next based on the internal logic shown in Figure 1.
    The model achieves adaptation by tweaking the alpha parameter <ALPHA> based on the learner's performance. Correctly guessing a word during one of the encounters means that the learner is making progress in strenghtening that word's activation in their memory. In that case the alpha gets reduced, which results in a lower decay and therefore the item is shown less often during the study session. If the reverse is the case (the learner gets the word wrong), then they need to practice it more in order to learn it better. To make sure this happens, the model increases the alpha, which in turn increases the decay and results in more frequent presentations. Furthermore, the model tries to present words before they fall below a forgetting threshold <BOLD tau BOLD>, at which point they would be forgotten by the user and previous learning will have been in vain. After a state is reached, where the decay is so minimal that the item's strength in memory is considered stable, the item stops being presented (and is considered "learned") and a new item from the vocabulary list enters the system.

    Subsection: Enhancements of the Pavlik and Anderson Model
    The Pavlik and Anderson model has been used as a basis for a lot of the research which targets vocabulary learning (and factual learning in general). Extensions to the model have been made by Pavlik (2007); Van Rijn, Van Maanen, and Van Woudenberg, 2009; Nijboer 2011 and others. All of the proposed extensions keep the general idea of the model, but aim to improve the activation calculation to either better represent the learner or better show human learning and cognitive patterns.
    In 2007 Pavlik suggested an improvement to the activation formula by adding 3 additional <ITALIC BETA ITALIC> parameters (Formula 2.0). Each of those parameters would contain either item-specific or learner-specific information, which would better represent the learner's connection to the particular item. Since different people learn at different rates, the <ITALIC BETAs ITALIC> parameter was added in order to introduce the learner's learning ability to the activation calculation. In addition to that, certain words are more difficult to learn than others (either because of their morphology or their semantics). To account for that, Pavlik added the <ITALIC BETAi ITALIC> parameter to the formula as well. The first two parameters are then combined in the <ITALIC BETAs,i ITALIC> parameter in order to represent the relative difficulty of a given item for the particular learner. To add an additional dimension to the calculation, the <ITALIC bj ITALIC> parameter was added, which allows the system to apply scaling to different word presentations. Meaning that it can now evaluate word-pair presentations differently than presentations of the word in context, which allows for much more control over the learning process.
    Formula 2.0: <ACTIVATION CALCULATION + BETA PARAMS>
    Different approaches were taken by Van Rijn, Van Maanen, Van Woudenberg in 2009 and Nijboer in 2011. They proposed that the learner's reaction time be recorded and used when calculating the activation of each item. This suggestion is based on the idea that if the learner takes longer to finish the word encounter, then it is more difficult for them to recall the item from memory. Nijboer further expanded upon the idea of using reaction time by providing a way to predict the expected time needed to process the item presentation exercise (for example the time needed to read a sentence, before the learner has to attempt to retrieve a missing word from their memory).

    Subsection: The problem
    Even though most of the proposed extensions have been shown to provide improved effectiveness of the model, none of them tackle the problem of computational efficiency. When integrated in a real-life system, an inefficient algorithm could inconvenience the user by making them wait while the activations of all items are calculated before each encounter, wasting precious learning time during the study sessions. Furthermore, since the activation calculation takes into account all previous encounters with the item, it becomes increasingly expensive when it comes to learning more difficult words (since they would take more encounters to learn). All of these factors have inspired this attempt at improving the computation time of the algorithm.


    Subsection: Program optimization methods
    The process of algorithm optimization usually aims to improve an algorithm by reducing the usage of a certain resource (whether it is execution time, hard drive space or another available resource). However, sometimes optimization with regards to one resource requires increased usage of another one. In the case of this learning algorithm, the valued resource is execution time (since the algorithm is executed before each item encounter and needs to quickly determine which word should be presented next).
    Program optimization is usually applied using established and proven methods. Those methods differ in the level at which the optimization occurs, as well as in the type of optimization problem they are trying to solve. The different levels of program optimization include:
    1. Optimal choice of technologies
    2. Algorithm design with attention to resource efficiency
    3. Writing source code with attention to efficient command execution
    4. Compiler optimizations
    The higher levels of optimization (where most of the work is usually done by the developer instead of the computer) tend to have a greater impact on program efficiency than the lower ones. This means that an inefficient algorithm, which was run on a highly optimized system will execute slower than an efficient algorithm, which was run on a sub-optimal system. Therefore the higher levels are usually targeted first when an optimization problem occurs.
    
    Some of the most famous optimization methods make use of recursion in order to simplify the initial problem into smaller, more manageable sub-problems. One such method is the well known <ITALIC Divide and conquer ITALIC>, which splits the initial problem recursively until the sub-problems are simple enough to be easily solved. After the base problems are solved, their solutions are combined in a way that yields the solution to the initial problem. Divide and conquer algorithms have found a wide usage from sorting problems to syntactic analysis problems {Reference: Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest, Introduction to Algorithms (MIT Press, 2000)}. Another optimization technique, which tries to optimize recursively solvable problems is <ITALIC Dynamic programming ITALIC>.
    While Divide and conquer is only suitable for problems, where the merging of the solutions to the recursively created sub-problems yields the result of the original problem, Dynamic programming is used in cases, where those solutions can be directly used to solve the original problem. The applicability of Dynamic programming techniques to the problem at hand depends on 2 properties: optimal substructure and overlapping sub-problems. Optimal substructure is fulfilled when an optimal solution to the problem contains optimal solutions to its sub-problems {ASTERISK: notice that when talking about optimization, there is no single optimal solution, there are multiple possible ones}. The other property of Dynamic programming refers to the fact that problems, to which it is applied do not constantly generate new sub-problems at each step (which is what Divide and conquer algorithms tend to do), but instead repeatedly solve the same problems. Since this repeated solving is very inefficient, Dynamic programming promotes the use of <ITALIC Memoization ITALIC> in order to store the results of sub-problems in a map so that the algorithm doesn't need to re-solve them every time they come up, but instead just fetch the solution from memory {Reference: Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest, Introduction to Algorithms (MIT Press, 2000)}.
    An example of a problem, which 

    // TODO: downloaded wiki articles (Dynamic computing, Divide and conquer, Approximate computing, Memoization)

    // TODO: add more references to linguistic papers


3. Optimization process



4. Evaluation of Performance (Setup + Results)



5. Possible extensions/improvements (Outlook)



6. Conclusion
