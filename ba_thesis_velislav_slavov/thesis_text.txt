Title: Optimized computational modeling of adaptive activation-based concept learning

TODO: sequences and series + recursion



1. Introduction
    (REDO)
    Technology has made its way into our working environments, vehicles, mobile phones and even into our homes, in an attempt to make our lives easier and more convenient. One area of our lives, where technology has been gaining popularity is education. Educational technology in particular has been gaining increasing popularity due to its efforts in improving the application of technology in facilitating learning.
    Learning words in languages = very important, declarative memory? A language user's mothertongue is learned at a very young age when not much conscious effort is put into the learning process, but instead the learner acquires the language unconsciously therefore that is referred to as "language acquisition". On the other hand when it comes to learning further foreign languages, the user has to actively participate in the learning process in which case we speak of "language learning".
    Cognitive psychology has been able to identify a number of patterns, called cognitive biases, which try to explain how humans process information, such as the modality effect, the testing effect and the spacing effect. While being aware of these patterns allows educators to simply develop systems, which reliably produce improved learning results, computers require well defined and formalized rules in order to achieve this. One of the more famous and successful cognitive architectures is the Adaptive Control of Thoughtâ€”Rational (known as ACT-R).
    The goal of this thesis is to analyze an adaptive activation-based learning model, based on the ACT-R architecture and try to develop a more computationally efficient implementation of that model. I will compare the perforamnce as well as the accuracy of the resulting implementation to the original implementation of the model.



2. Background
    Subsection: The spacing and testing effects
    When it comes to factual learning tasks, massed learning (colloquially known as cramming) has become the go-to approach. It consists of exposing the learner to a big amount of information for a short amount of time in an attempt to prepare the learner for an upcoming examination on the studied material. Research, however, has shown (Ebbinghaus, 1885) that massed learning only produces short-term results and therefore is unsuitable when we want to achieve long-term information retention. The proposed alternative for effective long-term results is the so-called distributed (or spaced) learning, which is based on Ebbinghaus' conclusions that spacing the individual item interactions further apart actually produces consistent long-term retention. This discovery is known as the "spacing effect". Furthermore, spaced learning attempts to optimize both intra-session intervals (how often each word gets presented within a study session) and inter-session intervals (how far apart different study sessions should take place).

    Subsection: The basics of ACT-R
    Since the model, which is being analyzed in this thesis is based on the ACT-R cognitive architecture, I will provide a very superficial introduction to it in order to introduce some important terminology and concepts. ACT-R stands for "Adaptive Character of Thought - Rational" and its aim is to explain and formally define how humans perceive and express both declarative and procedural knowledge. Declarative knowledge refers to the ability of the human brain to retrieve <BOLD factual information BOLD>, whereas procedural knowledge is connected to the subconscious ability to accomplish somewhat <BOLD complex procedures BOLD> (riding a bicycle, going up the stairs, writing a text etc.). Since vocabulary learning can be compared to learning facts, the model described in this thesis will make use of the implementation of declarative memory in the ACT-R architecture. The model is also referred to as an activation-based one, as it relies on the concept of "activations", which ACT-R uses in order to represent how strongly each memory item is embedded into memory. {Reference: van Woundenberg 2008}.

    Subsection: The Pavlik and Anderson model
    Pavlik and Anderson have managed to formalize this research into their model by modifying the base ACT-R formula for calculating activations. Formula 1.0 shows the formula developed by Pavlik and Anderson for calculating the activation of an item at a given time <ITALIC t ITALIC>. An item's activation is expressed as a power function of the sum of the time deltas of its previous encounters (how long ago each encounter occured), scaled by the decay rate at the given encounter (how fast the activation decays with the passage of time). This results in the latest encounters having the highest impact on the item's current activation.
    Formula 1.0: <ACTIVATION CALCULATION FORMULA>
    Decay for the current encounter is calculated by taking the activation of the item at the last encounter (formula 1.1). If a previous encounter happened when the item's activation was high, this would also produce a high rate of decay, which when plugged into the activation formula would further scale down the time difference, resulting in a smaller contribution of that encounter to the current activation of the item (meaning that being presented with a recently encountered item has little effect when it comes to strenghtening that item in memory). Decay can further be modified using the other two variables seen in the formula. The <ITALIC c ITALIC> parameter represents a scaling factor, which governs how strong the spacing effect is and the <ITALIC ALPHA ITALIC> parameter represents the lowest possible decay value (usually 0.5 since that is the default decay value in ACT-R). The formula recurses down for each previous encounter and the base case for each item occurs when the activation is being calculated for a time <ITALIC t ITALIC> when there are no previous encounters. In that case the result is an activation of negative infinity (-INF), since the item has never been encountered before and therefore has no strength whatsoever in the learner's memory. If decay is being calculated for an activation of -INF, this results in the default decay value.
    Formula 1.1: <DECAY CALCULATION FORMULA>
    Having calculated the activations of all items in the current vocabulary list at the current time, the model decides which item to present next based on the internal logic shown in Figure 1.
    The model achieves adaptation by tweaking the alpha parameter <ALPHA> based on the learner's performance. Correctly guessing a word during one of the encounters means that the learner is making progress in strenghtening that word's activation in their memory. In that case the alpha gets reduced, which results in a lower decay and therefore the item is shown less often during the study session. If the reverse is the case (the learner gets the word wrong), then they need to practice it more in order to learn it better. To make sure this happens, the model increases the alpha, which in turn increases the decay and results in more frequent presentations. Furthermore, the model tries to present words before they fall below a forgetting threshold <BOLD tau BOLD>, at which point they would be forgotten by the user and previous learning will have been in vain. After a state is reached, where the decay is so minimal that the item's strength in memory is considered stable, the item stops being presented (and is considered "learned") and a new item from the vocabulary list enters the system.

    Subsection: Enhancements of the Pavlik and Anderson Model
    The Pavlik and Anderson model has been used as a basis for a lot of the research which targets vocabulary learning (and factual learning in general). Extensions to the model have been made by Pavlik (2007); Van Rijn, Van Maanen, and Van Woudenberg, 2009; Nijboer 2011 and others. All of the proposed extensions keep the general idea of the model, but aim to improve the activation calculation to either better represent the learner or better show human learning patterns.
    In 2007 Pavlik suggested an improvement to the activation formula by adding 3 additional <ITALIC BETA ITALIC> parameters (Formula 2.0). Each of those parameters would contain either item-specific or learner-specific information, which would better represent the learner's connection to the particular item. Since different people learn at different rates, the <ITALIC BETAs ITALIC> parameter was added in order to introduce the learner's learning ability to the activation. In addition to that, certain words are more difficult to learn than others (either because of their morphology or their semantics). To account for that, Pavlik added the <ITALIC BETAi ITALIC> parameter to the formula as well. The first two parameters are then combined in the <ITALIC BETAs,i ITALIC> parameter in order to represent the relative difficulty of a given item with relation to the particular learner.
    Formula 2.0: <ACTIVATION CALCULATION + BETA PARAMS>

    Subsection: The problem

    Subsection: Program optimization
    //TODO: downloaded wiki articles


3. Optimization process



4. Evaluation of Performance (Setup + Results)



5. Possible extensions/improvements (Outlook)



6. Conclusion
